{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ultraleap_demo.training_history import *\n",
    "from ultraleap_demo.load_demo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dropout = 0.3\n",
    "\n",
    "# Load the models\n",
    "timeseries_models = {\n",
    "    16: {\n",
    "        1: None,\n",
    "        4: None,\n",
    "        8: None,\n",
    "        12: None\n",
    "    },\n",
    "    32: {\n",
    "        1: None,\n",
    "        8: None,\n",
    "        16: None,\n",
    "        24: None\n",
    "    }\n",
    "}\n",
    "for model_file in MODEL_FILES:\n",
    "    keyname = model_file.split(\"\\\\\")[-1].split(\"step_\")[-1].split(\"_batch\")[0]\n",
    "    sequence_length = int(keyname.split(\"sequence_\")[-1].split(\"_\")[0])\n",
    "    output_window = int(keyname.split(\"output_\")[-1].split(\"_\")[0])\n",
    "    moving_ntokens = ntokens[sequence_length][output_window][\"moving\"]\n",
    "    palm_ntokens = ntokens[sequence_length][output_window][\"palm\"]\n",
    "    hand_ntokens = ntokens[sequence_length][output_window][\"hand\"]\n",
    "    feature_ntokens = [moving_ntokens, palm_ntokens, hand_ntokens]\n",
    "\n",
    "    model = TransformerModel(len(feature_ntokens), feature_ntokens, d_model, nhead, num_layers, max_len = sequence_length)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "\n",
    "    timeseries_models[sequence_length][output_window] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {\"timeseries\":timeseries_models, \"classifier\":classifier_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sequence(sequence, target_length):\n",
    "    normalized_sequence = []\n",
    "    sequence_length = len(sequence)\n",
    "    \n",
    "    for i in range(target_length):\n",
    "        index = int((i / target_length) * sequence_length)\n",
    "        normalized_sequence.append(sequence[index])\n",
    "    \n",
    "    return np.array(normalized_sequence)\n",
    "\n",
    "\n",
    "def get_predict_sequence(mapped_moving_directions, mapped_palm_orientations, mapped_similarity_states, sequence_length, output_window):\n",
    "    combined = combine_mapped_separated_sequences([mapped_moving_directions], [mapped_palm_orientations], [mapped_similarity_states])[0][output_window-sequence_length:]\n",
    "    to_predict_sequence = make_predict_frame_sequence(combined, sequence_length, output_window)\n",
    "    return to_predict_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add every n frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from dtw import dtw\n",
    "import keyboard\n",
    "import copy\n",
    "\n",
    "import leap\n",
    "import time\n",
    "\n",
    "orientation_mapping = {'up': 0, 'down': 1, 'opposite': 2}\n",
    "inverted_orientation_mapping = {v: k for k, v in orientation_mapping.items()}\n",
    "direction_mapping_2d = {'up': 0, 'down': 1, 'left': 2, 'right': 3, 'stationary': 4}\n",
    "inverted_direction_mapping_2d = {v: k for k, v in direction_mapping_2d.items()}\n",
    "\n",
    "gesture_mapped_names = {\n",
    "    \"gesture_7\":\"Swipe Right\",\n",
    "    \"gesture_8\":\"Swipe Left\",\n",
    "    \"gesture_9\":\"Swipe Up\",\n",
    "    \"gesture_10\":\"Swipe Down\",\n",
    "}\n",
    "\n",
    "top_dirs = 3\n",
    "window_size = 7 # Original data used 30 fps camera, Leap 2 uses 120 fps camera so we need to multiply by 4\n",
    "stationary_threshold_ratio = 1.5\n",
    "moving_percentage = 0.45\n",
    "# moving_direction_indexes = [0, 1, 2, 6, 18, 21, 5, 13, 9, 17, 10, 14]\n",
    "moving_direction_indexes = None\n",
    "\n",
    "similarity_lookback = 4\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "sequence_length = 16\n",
    "output_window = 1\n",
    "max_frames = 100\n",
    "add_every_n_frame = 10\n",
    "frame_num = 0\n",
    "interpolated_frame_num = 0\n",
    "target_length = 64\n",
    "\n",
    "frames = Frames(\n",
    "    handpose=handpose_filtered, \n",
    "    sequence_length=sequence_length, \n",
    "    max_frames=max_frames, \n",
    "    window_size=window_size, \n",
    "    similarity_lookback=similarity_lookback,\n",
    "    stationary_threshold_ratio=stationary_threshold_ratio,\n",
    "    moving_direction_indexes=moving_direction_indexes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    moving_direction_mapping=moving_direction_state_mapping, \n",
    "    palm_orientation_mapping=palm_orientation_state_mapping, \n",
    "    hand_pose_mapping=hand_pose_state_mapping)\n",
    "\n",
    "\n",
    "timeseries_model = all_models[\"timeseries\"][sequence_length][output_window]\n",
    "classifier_model = list(all_models[\"classifier\"].values())[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyListener(leap.Listener):\n",
    "    def on_connection_event(self, event):\n",
    "        print(\"Connected\")\n",
    "\n",
    "    def on_device_event(self, event):\n",
    "        try:\n",
    "            with event.device.open():\n",
    "                info = event.device.get_info()\n",
    "        except leap.LeapCannotOpenDeviceError:\n",
    "            info = event.device.get_info()\n",
    "\n",
    "        print(f\"Found device {info.serial}\")\n",
    "\n",
    "    def on_tracking_event(self, event):\n",
    "        global frame_num, interpolated_frame_num\n",
    "        frame_num += 1\n",
    "        for hand in event.hands:\n",
    "\n",
    "            if frame_num % add_every_n_frame == 0:\n",
    "                interpolated_frame_num += 1\n",
    "                landmarks = Landmarks(hand)\n",
    "                frame = Frame(landmarks)\n",
    "\n",
    "                frames.add_frame(frame, True)\n",
    "                print(f\"Added frame {interpolated_frame_num}\")\n",
    "\n",
    "\n",
    "        if frame_num % add_every_n_frame == 0:\n",
    "            if (len(frames.mapped_moving_directions) > 0) and len(event.hands) > 0:\n",
    "                print(f\"moving: {inverted_direction_mapping_2d[int(inverted_moving_direction_state_mapping[frames.mapped_moving_directions[-1]])]}\")\n",
    "                print(f\"orientation: {inverted_orientation_mapping[int(inverted_palm_orientation_state_mapping[frames.mapped_palm_orientations[-1]])]}\")\n",
    "                print(f\"hand pose: {frames.similarity_states[-1]}\")\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        #         # count the number of '4' in the mapped_moving_direction[-sequence_length:] (stationary state)\n",
    "        #         stationary_count = frames.mapped_moving_directions[-sequence_length:].count(4)\n",
    "\n",
    "        #         if stationary_count <= sequence_length * moving_percentage:\n",
    "\n",
    "        #             mapped_moving_directions = frames.mapped_moving_directions\n",
    "        #             mapped_palm_orientations = frames.mapped_palm_orientations\n",
    "        #             mapped_similarity_states = frames.mapped_similarity_states\n",
    "\n",
    "        #             combined = combine_mapped_separated_sequences([mapped_moving_directions], [mapped_palm_orientations], [mapped_similarity_states])[0][output_window-sequence_length:]\n",
    "        #             to_predict_sequence = make_predict_frame_sequence(combined, sequence_length, output_window)\n",
    "        #             predicted_states = combine_predicted_features(make_prediction(timeseries_model, to_predict_sequence, output_window))\n",
    "\n",
    "        #             performed_states = to_predict_sequence.tolist()[0][:-output_window]\n",
    "        #             combined_states = performed_states + predicted_states\n",
    "\n",
    "        #             print(combined_states)\n",
    "\n",
    "        #             norm_combined_states = torch.tensor(normalize_sequence(combined_states, target_length), dtype=torch.float32).unsqueeze(0).to('cpu')\n",
    "        #             pred = torch.max(classifier_model(norm_combined_states), 1)\n",
    "        #             pred_score = pred[0].item()\n",
    "        #             pred_index = pred[1].item()\n",
    "        #             mapped_pred = gesture_mapped_names[classifier_mappings[\"inverted_gesture_state_mapping\"][pred_index]]\n",
    "        #             if pred_score > 10:\n",
    "        #                 print(f\"Predicted: {mapped_pred} with score {pred_score}\")\n",
    "                    \n",
    "        #             clear_output(wait=True)\n",
    "\n",
    "        #         else:\n",
    "        #             print(\"Stationary\")\n",
    "        #             clear_output(wait=True)\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "def main():\n",
    "    my_listener = MyListener()\n",
    "\n",
    "    connection = leap.Connection()\n",
    "    connection.add_listener(my_listener)\n",
    "\n",
    "    running = True\n",
    "\n",
    "    with connection.open():\n",
    "        connection.set_tracking_mode(leap.TrackingMode.Desktop)\n",
    "        while running:\n",
    "            if keyboard.is_pressed('q'):\n",
    "                running = False\n",
    "                break\n",
    "            # if keyboard.is_pressed('a'):\n",
    "                \n",
    "            if interpolated_frame_num >= max_frames:\n",
    "                running = False\n",
    "                break\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add frame with 'space' bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from dtw import dtw\n",
    "import keyboard\n",
    "import copy\n",
    "\n",
    "import leap\n",
    "import time\n",
    "\n",
    "orientation_mapping = {'up': 0, 'down': 1, 'opposite': 2}\n",
    "inverted_orientation_mapping = {v: k for k, v in orientation_mapping.items()}\n",
    "direction_mapping_2d = {'up': 0, 'down': 1, 'left': 2, 'right': 3, 'stationary': 4}\n",
    "inverted_direction_mapping_2d = {v: k for k, v in direction_mapping_2d.items()}\n",
    "\n",
    "gesture_mapped_names = {\n",
    "    \"gesture_7\":\"Swipe Right\",\n",
    "    \"gesture_8\":\"Swipe Left\",\n",
    "    \"gesture_9\":\"Swipe Up\",\n",
    "    \"gesture_10\":\"Swipe Down\",\n",
    "}\n",
    "\n",
    "top_dirs = 2\n",
    "window_size = 7 # Original data used 30 fps camera, Leap 2 uses 120 fps camera so we need to multiply by 4\n",
    "stationary_threshold_ratio = 1\n",
    "moving_percentage = 0.45\n",
    "# moving_direction_indexes = [0, 1, 2, 6, 18, 21, 5, 13, 9, 17, 10, 14]\n",
    "moving_direction_indexes = None\n",
    "\n",
    "similarity_lookback = 4\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "sequence_length = 16\n",
    "output_window = 1\n",
    "max_frames = 100\n",
    "add_every_n_frame = 4\n",
    "frame_num = 0\n",
    "added_frame_num = 0\n",
    "previous_added_frame_num = 0\n",
    "\n",
    "\n",
    "target_length = 64\n",
    "\n",
    "frames = Frames(\n",
    "    handpose=handpose_filtered, \n",
    "    sequence_length=sequence_length, \n",
    "    max_frames=max_frames, \n",
    "    window_size=window_size, \n",
    "    similarity_lookback=similarity_lookback,\n",
    "    stationary_threshold_ratio=stationary_threshold_ratio,\n",
    "    moving_direction_indexes=moving_direction_indexes,\n",
    "    similarity_threshold=similarity_threshold,\n",
    "    moving_direction_mapping=moving_direction_state_mapping, \n",
    "    palm_orientation_mapping=palm_orientation_state_mapping, \n",
    "    hand_pose_mapping=hand_pose_state_mapping)\n",
    "\n",
    "\n",
    "timeseries_model = all_models[\"timeseries\"][sequence_length][output_window]\n",
    "classifier_model = list(all_models[\"classifier\"].values())[0]\n",
    "\n",
    "# def increment_frame_num():\n",
    "#     global added_frame_num\n",
    "#     added_frame_num += 1\n",
    "#     print(f'Counter: {added_frame_num}')\n",
    "\n",
    "\n",
    "class MyListener(leap.Listener):\n",
    "    def on_connection_event(self, event):\n",
    "        print(\"Connected\")\n",
    "\n",
    "    def on_device_event(self, event):\n",
    "        try:\n",
    "            with event.device.open():\n",
    "                info = event.device.get_info()\n",
    "        except leap.LeapCannotOpenDeviceError:\n",
    "            info = event.device.get_info()\n",
    "\n",
    "        print(f\"Found device {info.serial}\")\n",
    "\n",
    "    def on_tracking_event(self, event):\n",
    "        global frame_num, added_frame_num, previous_added_frame_num\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "        \n",
    "\n",
    "        for hand in event.hands:\n",
    "            if keyboard.is_pressed('space') and frame_num % add_every_n_frame == 0:\n",
    "                added_frame_num += 1\n",
    "                if added_frame_num != previous_added_frame_num:\n",
    "                    landmarks = Landmarks(hand)\n",
    "                    frame = Frame(landmarks)\n",
    "\n",
    "                    frames.add_frame(frame, True)\n",
    "                    print(f\"Added frame {added_frame_num}\")\n",
    "\n",
    "                    previous_added_frame_num = added_frame_num\n",
    "\n",
    "\n",
    "        if (len(frames.mapped_moving_directions) > 0) and len(event.hands) > 0:\n",
    "            \n",
    "            print(f\"moving: {inverted_direction_mapping_2d[int(inverted_moving_direction_state_mapping[frames.mapped_moving_directions[-1]])]}\")\n",
    "            print(f\"orientation: {inverted_orientation_mapping[int(inverted_palm_orientation_state_mapping[frames.mapped_palm_orientations[-1]])]}\")\n",
    "            print(f\"hand pose: {frames.similarity_states[-1]}\")\n",
    "\n",
    "            to_predict = get_predict_sequence(frames.mapped_moving_directions, frames.mapped_palm_orientations, frames.similarity_states, sequence_length, output_window)\n",
    "            # clear_output(wait=True)\n",
    "            predicted_states = combine_predicted_features(make_prediction(timeseries_model, to_predict, output_window))\n",
    "\n",
    "            performed_states = to_predict.tolist()[0][:-output_window]\n",
    "            combined_states = performed_states + predicted_states\n",
    "\n",
    "            print(combined_states)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    global added_frame_num\n",
    "\n",
    "    my_listener = MyListener()\n",
    "\n",
    "    connection = leap.Connection()\n",
    "    connection.add_listener(my_listener)\n",
    "\n",
    "    running = True\n",
    "    with connection.open():\n",
    "        connection.set_tracking_mode(leap.TrackingMode.Desktop)\n",
    "        while running:\n",
    "            if keyboard.is_pressed('q'):\n",
    "                running = False\n",
    "                break\n",
    "            \n",
    "                \n",
    "            if (len(frames.mapped_moving_directions) > 64):\n",
    "                running = False\n",
    "                break\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare time series prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_frame_sequence(mapped_frame_sequence, sequence_length, output_window):\n",
    "    if len(mapped_frame_sequence) == sequence_length - output_window:\n",
    "        return torch.tensor([mapped_frame_sequence + [[0, 0, 0]]*output_window]).to(device)\n",
    "    elif len(mapped_frame_sequence) == sequence_length:\n",
    "        return torch.tensor([mapped_frame_sequence[:-output_window] + [[0, 0, 0]]*output_window]).to(device)\n",
    "    else:\n",
    "        if len(mapped_frame_sequence) > sequence_length - output_window:\n",
    "            return torch.tensor([mapped_frame_sequence[-(sequence_length - output_window):] + [[0, 0, 0]]*output_window]).to(device)\n",
    "        else:\n",
    "            return torch.tensor([[[0, 0, 0]]*(sequence_length - len(mapped_frame_sequence) - output_window) + mapped_frame_sequence + [[0, 0, 0]]*output_window]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_moving_directions = frames.mapped_moving_directions\n",
    "mapped_palm_orientations = frames.mapped_palm_orientations\n",
    "mapped_similarity_states = frames.similarity_states\n",
    "\n",
    "\n",
    "target_length = 64\n",
    "sequence_length = 16\n",
    "output_window = 12\n",
    "\n",
    "timeseries_model = all_models[\"timeseries\"][sequence_length][output_window]\n",
    "classifier_model = list(all_models[\"classifier\"].values())[0]\n",
    "\n",
    "combined = combine_mapped_separated_sequences([mapped_moving_directions], [mapped_palm_orientations], [mapped_similarity_states])[0]\n",
    "combined_normalized = normalize_sequence(combined, sequence_length)\n",
    "print(combined_normalized.tolist())\n",
    "true_state = np.array(combined_normalized.tolist()[-output_window:])\n",
    "\n",
    "to_predict_sequence = make_predict_frame_sequence(combined_normalized.tolist(), sequence_length, output_window)\n",
    "print(to_predict_sequence.tolist())\n",
    "\n",
    "predicted_states = combine_predicted_features(make_prediction(timeseries_model, to_predict_sequence, output_window))\n",
    "performed_states = to_predict_sequence.tolist()[0][:-output_window]\n",
    "combined_states = performed_states + predicted_states\n",
    "print(combined_states)\n",
    "predicted_states = np.array(predicted_states)\n",
    "feature_wise_accuracy = (true_state == predicted_states).sum() / (true_state.shape[1] * true_state.shape[0])\n",
    "print(f\"Feature-wise accuracy: {feature_wise_accuracy}\")\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for truth, pred in zip(true_state, predicted_states):\n",
    "    if np.array_equal(truth, pred):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"State-wise accuracy: {correct}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_moving_directions = frames.mapped_moving_directions\n",
    "mapped_palm_orientations = frames.mapped_palm_orientations\n",
    "mapped_similarity_states = frames.similarity_states\n",
    "\n",
    "\n",
    "target_length = 64\n",
    "sequence_length = 32\n",
    "output_window = 1\n",
    "\n",
    "timeseries_model = all_models[\"timeseries\"][sequence_length][output_window]\n",
    "classifier_model = list(all_models[\"classifier\"].values())[0]\n",
    "\n",
    "combined = combine_mapped_separated_sequences([mapped_moving_directions], [mapped_palm_orientations], [mapped_similarity_states])[0][-sequence_length:]\n",
    "# print(combined_normalized.tolist())\n",
    "print(len(combined))\n",
    "true_state = np.array(combined[-output_window:])\n",
    "print(true_state.tolist())\n",
    "\n",
    "to_predict_sequence = make_predict_frame_sequence(combined, sequence_length, output_window)\n",
    "print(to_predict_sequence.tolist())\n",
    "\n",
    "predicted_states = combine_predicted_features(make_prediction(timeseries_model, to_predict_sequence, output_window))\n",
    "performed_states = to_predict_sequence.tolist()[0][:-output_window]\n",
    "combined_states = performed_states + predicted_states\n",
    "print(combined_states)\n",
    "predicted_states = np.array(predicted_states)\n",
    "feature_wise_accuracy = (true_state == predicted_states).sum() / (true_state.shape[1] * true_state.shape[0])\n",
    "state_wise_accuracy = (true_state == predicted_states).sum() / true_state.size\n",
    "print(f\"Feature-wise accuracy: {feature_wise_accuracy}\")\n",
    "print(f\"State-wise accuracy: {state_wise_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames.mapped_moving_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyboard\n",
    "\n",
    "# Initialize the variable\n",
    "# counter = 0\n",
    "\n",
    "def increment_counter():\n",
    "    global counter\n",
    "    counter += 1\n",
    "    print(f'Counter: {counter}')\n",
    "\n",
    "# Add hotkey\n",
    "keyboard.on_release_key('enter', increment_counter())\n",
    "\n",
    "# Block forever, to keep the script running.\n",
    "# keyboard.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Uses interpolation in Leap API to determine the location of hands based on \n",
    "previous data. We use the LatestEventListener to wait until we have tracking \n",
    "events. We delay by 0.02 seconds each frame to simulate some delay, we get a \n",
    "frame size of the frame closest to the time we want to interpolate from and \n",
    "then interpolate on that frame\"\"\"\n",
    "import leap\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Callable\n",
    "from leap.events import TrackingEvent\n",
    "from leap.event_listener import LatestEventListener\n",
    "from leap.datatypes import FrameData\n",
    "\n",
    "\n",
    "def wait_until(condition: Callable[[], bool], timeout: float = 5, poll_delay: float = 0.01):\n",
    "    start_time = timer()\n",
    "    while timer() - start_time < timeout:\n",
    "        if condition():\n",
    "            return True\n",
    "        time.sleep(poll_delay)\n",
    "    if not condition():\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    tracking_listening = LatestEventListener(leap.EventType.Tracking)\n",
    "\n",
    "    connection = leap.Connection()\n",
    "    connection.add_listener(tracking_listening)\n",
    "\n",
    "    with connection.open() as open_connection:\n",
    "        wait_until(lambda: tracking_listening.event is not None)\n",
    "        # ctr-c to exit\n",
    "        while True:\n",
    "            event = tracking_listening.event\n",
    "            if event is None:\n",
    "                continue\n",
    "            event_timestamp = event.timestamp\n",
    "\n",
    "            target_frame_size = leap.ffi.new(\"uint64_t*\")\n",
    "            frame_time = leap.ffi.new(\"int64_t*\")\n",
    "            frame_time[0] = event_timestamp\n",
    "\n",
    "            # simulate 20 ms delay\n",
    "            time.sleep(0.02)\n",
    "\n",
    "            try:\n",
    "                # we need to query the storage required for our interpolation\n",
    "                # request, the size will depend on the number visible hands in\n",
    "                # this frame\n",
    "                leap.get_frame_size(open_connection, frame_time, target_frame_size)\n",
    "            except Exception as e:\n",
    "                print(\"get_frame_size() failed with: \", e)\n",
    "                continue\n",
    "\n",
    "            frame_data = FrameData(target_frame_size[0])\n",
    "            try:\n",
    "                # actually interpolate and get frame data from the Leap API\n",
    "                # this is the time of the frame plus the 20ms artificial\n",
    "                # delay and an estimated 10ms processing time which should\n",
    "                # get close to real time hand tracking with interpolation\n",
    "                leap.interpolate_frame(\n",
    "                    open_connection,\n",
    "                    event_timestamp + 40000,\n",
    "                    frame_data.frame_ptr(),\n",
    "                    target_frame_size[0],\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"interpolate_frame() failed with: \", e)\n",
    "                continue\n",
    "\n",
    "            event = TrackingEvent(frame_data)\n",
    "            print(\n",
    "                \"Frame \",\n",
    "                event.tracking_frame_id,\n",
    "                \" with \",\n",
    "                len(event.hands),\n",
    "                \"hands with a delay of \",\n",
    "                leap.get_now() - event.timestamp,\n",
    "            )\n",
    "            for hand in event.hands:\n",
    "                hand_type = \"left\" if str(hand.type) == \"HandType.Left\" else \"right\"\n",
    "                print(\n",
    "                    f\"Hand id {hand.id} is a {hand_type} hand with position ({hand.palm.position.x}, {hand.palm.position.y}, {hand.palm.position.z}).\"\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install traitlets==5.1.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
