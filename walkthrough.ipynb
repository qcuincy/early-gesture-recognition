{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting discrete features from 3D Hand landmark coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook aims to cover the following:\n",
    "\n",
    "1. **Retrieving data from the \"hand_gesture_data\" folder in the current directory.**\n",
    "    - Users have the option to collect their own data using the \"data_collect.py\" script or use the existing example data.\n",
    "2. **Extracting discrete features from the coordinates** \n",
    "    1. **Palm Orientation**\n",
    "    2. **Moving Direction**\n",
    "    3. **Hand Pose**\n",
    "\n",
    "3. **Classifying dynamic hand gestures using the sequences of discrete features (Optional)** \n",
    "    - These features can be used for classifying dynamic hand gestures or training a new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Retrieving data from the \"hand_gesture_data\" folder**\n",
    "- Inside the \"hand_gesture_data\" folder, there are subfolders named after the hand gestures.\n",
    "- Each subfolder contains the data for a specific hand gesture.\n",
    "- You can use the existing example data or collect your own data using the \"data_collect.py\" script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load the existing example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of swipe right trials: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data = os.path.join(os.getcwd(), \"hand_gesture_data\")\n",
    "\n",
    "swipe_right = os.path.join(data, \"swipe_right\")\n",
    "swipe_right_files = [os.path.join(swipe_right, f) for f in os.listdir(swipe_right)]\n",
    "\n",
    "swipe_right_data = {\n",
    "    \"features\": [np.load(f) for f in swipe_right_files if (\"_hands\" not in f and \"_raw\" not in f)],\n",
    "    \"raw\": [np.load(f) for f in swipe_right_files if \"_raw\" in f],\n",
    "    \"hands\": [np.load(f, allow_pickle=True) for f in swipe_right_files if \"_hands\" in f]\n",
    "}\n",
    "\n",
    "print(f\"Number of swipe right trials: {len(swipe_right_data['features'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Store the data in some accessible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 number of recorded frames: 227\n"
     ]
    }
   ],
   "source": [
    "trial = 1\n",
    "\n",
    "swipe_right_trial = {\n",
    "    \"features\": swipe_right_data[\"features\"][trial],\n",
    "    \"raw\": swipe_right_data[\"raw\"][trial],\n",
    "    \"hands\": swipe_right_data[\"hands\"][trial]\n",
    "}\n",
    "print(f\"Trial {trial} number of recorded frames: {len(swipe_right_trial['features'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To visualise the data, run the cell below and adjust the slider to view the hand landmarks for different frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293697c4265c4c7db19463c32fc6972a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', layout=Layout(width='90%'), max=225), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_frames(frame_number)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import cv2\n",
    "\n",
    "from ultraleap_demo.hand_renderer import HandRenderer # This is a simple class to render the recorded hand gesture data frame by frame\n",
    "\n",
    "# Load the hand data\n",
    "hand_data = swipe_right_trial[\"hands\"]\n",
    "hands_colour = (255, 255, 255)\n",
    "\n",
    "# Create the instances of the HandRenderer class for dots and skeleton formats\n",
    "dots_renderer = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Dots\", hands_colour=hands_colour, circle_radius=3)\n",
    "skeleton_renderer = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Skeleton\", hands_colour=hands_colour, circle_radius=3)\n",
    "\n",
    "# Create a slider for frame navigation\n",
    "frame_slider = widgets.IntSlider(min=0, max=len(hand_data)-1, step=1, value=0, layout=widgets.Layout(width='90%'), description='Frame')\n",
    "\n",
    "def show_frames(frame_number):\n",
    "    # Render the hand data for this frame\n",
    "    dots_renderer.render_hand_data(hand_data[frame_number])\n",
    "    skeleton_renderer.render_hand_data(hand_data[frame_number])\n",
    "\n",
    "    # Convert the images from BGR to RGB\n",
    "    dots_image_rgb = cv2.cvtColor(dots_renderer.output_image, cv2.COLOR_BGR2RGB)\n",
    "    skeleton_image_rgb = cv2.cvtColor(skeleton_renderer.output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Display the dots image\n",
    "    plt.imshow(skeleton_image_rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(\"Skeleton\")  # Set the title\n",
    "\n",
    "    # Display the skeleton image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(dots_image_rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(\"Dots\")  # Set the title\n",
    "\n",
    "    plt.suptitle(f\"Frame {frame_number}\", fontsize=16)  # Set the title for the whole figure\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display the slider and the frame\n",
    "interact(show_frames, frame_number=frame_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Extracting discrete features from the coordinates** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 **Palm Orientation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 **Functions** to calculate the palm orientation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultraleap_demo.prep_functions import *\n",
    "\n",
    "def rotate_vector(vector, axis, theta):\n",
    "    \"\"\"\n",
    "    Rotate a 3D vector around the x, y, or z axis by a given angle (theta).\n",
    "    \n",
    "    Parameters:\n",
    "        vector (ndarray): The 3D vector to rotate.\n",
    "        axis (str): The axis to rotate around ('x', 'y', or 'z').\n",
    "        theta (float): The angle (in radians) to rotate by.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: The rotated 3D vector.\n",
    "    \"\"\"\n",
    "    rotation_matrix = np.eye(3)\n",
    "    \n",
    "    if axis == 'x':\n",
    "        rotation_matrix[1, 1] = np.cos(theta)\n",
    "        rotation_matrix[1, 2] = -np.sin(theta)\n",
    "        rotation_matrix[2, 1] = np.sin(theta)\n",
    "        rotation_matrix[2, 2] = np.cos(theta)\n",
    "    elif axis == 'y':\n",
    "        rotation_matrix[0, 0] = np.cos(theta)\n",
    "        rotation_matrix[0, 2] = np.sin(theta)\n",
    "        rotation_matrix[2, 0] = -np.sin(theta)\n",
    "        rotation_matrix[2, 2] = np.cos(theta)\n",
    "    elif axis == 'z':\n",
    "        rotation_matrix[0, 0] = np.cos(theta)\n",
    "        rotation_matrix[0, 1] = -np.sin(theta)\n",
    "        rotation_matrix[1, 0] = np.sin(theta)\n",
    "        rotation_matrix[1, 1] = np.cos(theta)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid axis. Must be 'x', 'y', or 'z'.\")\n",
    "    \n",
    "    rotated_vector = np.dot(rotation_matrix, vector)\n",
    "    \n",
    "    return rotated_vector\n",
    "\n",
    "\n",
    "def get_average_landmarks(landmarks_frames):\n",
    "    # From the frames list of hand landmarks get the average position for each landmark\n",
    "    average_landmarks = np.zeros((22, 3))\n",
    "    for frame in landmarks_frames:\n",
    "        average_landmarks += frame\n",
    "    average_landmarks /= len(landmarks_frames)\n",
    "    return average_landmarks\n",
    "\n",
    "\n",
    "def get_normal_vector(landmarks):\n",
    "    # Extract the relevant landmarks (assuming 0-based indexing)\n",
    "    index_base = np.array(landmarks[6])  # Base of the index finger\n",
    "    pinky_base = np.array(landmarks[14])  # Base of the pinky finger\n",
    "    wrist = np.array(landmarks[1])  # Wrist\n",
    "\n",
    "    # Calculate the vectors between the points\n",
    "    v1 = index_base - wrist\n",
    "    v2 = pinky_base - wrist\n",
    "\n",
    "    # Compute the normal vector of the palm plane\n",
    "    normal_vector = np.cross(v1, v2)\n",
    "\n",
    "    # Normalize the normal vector\n",
    "    normal_vector = normal_vector / np.linalg.norm(normal_vector)\n",
    "\n",
    "    return normal_vector\n",
    "\n",
    "def classify_palm_rotation(palm_normal, camera_vector, threshold=0.666):\n",
    "    # Calculate the dot product between the palm normal and camera vector\n",
    "    dot_product = np.dot(palm_normal, camera_vector)\n",
    "\n",
    "    second_dot_product = np.dot(palm_normal, rotate_vector(camera_vector, 'y', np.pi/2))\n",
    "\n",
    "    third_dot_product = np.dot(palm_normal, rotate_vector(camera_vector, 'y', -np.pi/2))\n",
    "\n",
    "    condition_1 = second_dot_product > -threshold and second_dot_product < threshold\n",
    "    condition_2 = third_dot_product > -threshold and third_dot_product < threshold\n",
    "\n",
    "    product = second_dot_product * third_dot_product\n",
    "\n",
    "    # Classify the palm rotation based on the dot product value\n",
    "    if dot_product > threshold:\n",
    "        if condition_1 and condition_2:\n",
    "            if product > 0:\n",
    "                return \"down\"\n",
    "            else:\n",
    "                if (np.abs(product) < 0.1):\n",
    "                    return \"down\"\n",
    "                else:\n",
    "                    return \"up\"\n",
    "        else:\n",
    "            return \"down\"\n",
    "    elif dot_product < -threshold:\n",
    "        return \"up\"\n",
    "    elif second_dot_product > -threshold and second_dot_product < threshold:\n",
    "        if third_dot_product < threshold:\n",
    "            return \"opposite\"\n",
    "        else:\n",
    "\n",
    "            return \"down\"\n",
    "    else:\n",
    "        return \"opposite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 **Calculating** the palm orientation for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The palm orientation for the first frame is facing: down\n"
     ]
    }
   ],
   "source": [
    "hand_landmarks_frame = swipe_right_trial[\"raw\"][0]\n",
    "\n",
    "normal_vector = get_normal_vector(hand_landmarks_frame)\n",
    "\n",
    "result = classify_palm_rotation(normal_vector, np.array([0, 1, 0]))\n",
    "print(f\"The palm orientation for the first frame is facing: {result}\")\n",
    "\n",
    "# Now we can get the palm orientations for the rest of the frames\n",
    "palm_orientation_features = []\n",
    "for frame in swipe_right_trial[\"raw\"]:\n",
    "    normal_vector = get_normal_vector(frame)\n",
    "    result = classify_palm_rotation(normal_vector, np.array([0, 1, 0]))\n",
    "    palm_orientation_features.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'down' orientation is also the same as towards the camera. Similarly, the 'up' orientation is the same as away from the camera. The 'opposite' orientation is all the other orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/nBf02Jh/orientation-sphere-cropped.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 **Visualising** the palm orientation for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a2e991b7264f34bf1f01dc9a8c56c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', layout=Layout(width='90%'), max=225), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_frames(frame_number)>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import cv2\n",
    "\n",
    "from ultraleap_demo.hand_renderer import HandRenderer # This is a simple class to render the recorded hand gesture data frame by frame\n",
    "\n",
    "# Load the hand data\n",
    "hand_data = swipe_right_trial[\"hands\"]\n",
    "hands_colour = (255, 255, 255)\n",
    "\n",
    "# Create the instances of the HandRenderer class for dots and skeleton formats\n",
    "dots_renderer = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Dots\", hands_colour=hands_colour, circle_radius=3)\n",
    "skeleton_renderer = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Skeleton\", hands_colour=hands_colour, circle_radius=3)\n",
    "\n",
    "# Create a slider for frame navigation\n",
    "frame_slider = widgets.IntSlider(min=0, max=len(hand_data)-1, step=1, value=0, layout=widgets.Layout(width='90%'), description='Frame')\n",
    "\n",
    "def show_frames(frame_number):\n",
    "    # Render the hand data for this frame\n",
    "    dots_renderer.render_hand_data(hand_data[frame_number])\n",
    "    skeleton_renderer.render_hand_data(hand_data[frame_number])\n",
    "\n",
    "    # Convert the images from BGR to RGB\n",
    "    dots_image_rgb = cv2.cvtColor(dots_renderer.output_image, cv2.COLOR_BGR2RGB)\n",
    "    skeleton_image_rgb = cv2.cvtColor(skeleton_renderer.output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Display the dots image\n",
    "    plt.imshow(skeleton_image_rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(\"Skeleton\")  # Set the title\n",
    "\n",
    "    # Display the skeleton image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(dots_image_rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(\"Dots\")  # Set the title\n",
    "\n",
    "    plt.suptitle(f\"Frame {frame_number} - Palm Orientation: {palm_orientation_features[frame_number]}\", fontsize=16)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display the slider and the frame\n",
    "interact(show_frames, frame_number=frame_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 **Moving Direction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 **Functions** to calculate the moving direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_directions_sorted(vec, avg_distance, stationary_threshold_ratio=0.8, dimensions=2):\n",
    "\n",
    "    if dimensions == 3:\n",
    "        categories = {'up': 0, 'down': 0, 'towards_camera': 0, 'away_from_camera': 0, 'left': 0, 'right': 0, 'stationary': 0}\n",
    "        \n",
    "\n",
    "        stationary_threshold = avg_distance * stationary_threshold_ratio\n",
    "        x, y, z = vec\n",
    "        \n",
    "        if x > 0:\n",
    "            categories['right'] = x\n",
    "        else:\n",
    "            categories['left'] = x\n",
    "        if y > 0:\n",
    "            categories['up'] = y\n",
    "        else:\n",
    "            categories['down'] = y\n",
    "        if z > 0:\n",
    "            categories['towards_camera'] = z\n",
    "        else:\n",
    "            categories['away_from_camera'] = z\n",
    "        if np.linalg.norm(vec) < stationary_threshold:\n",
    "            categories['stationary'] = np.linalg.norm(vec)\n",
    "\n",
    "        # Get the key, value with the highest magnitude\n",
    "        \n",
    "    else:\n",
    "        categories = {'up': 0, 'down': 0, 'left': 0, 'right': 0, 'stationary': 0}\n",
    "        stationary_threshold = avg_distance * stationary_threshold_ratio\n",
    "        x, y, z = vec\n",
    "        if x > 0:\n",
    "            categories['right'] = x\n",
    "        else:\n",
    "            categories['left'] = x\n",
    "        if y > 0:\n",
    "            categories['up'] = y\n",
    "        else:\n",
    "            categories['down'] = y\n",
    "        if np.linalg.norm(vec) < stationary_threshold:\n",
    "            categories['stationary'] = np.linalg.norm(vec)\n",
    "    return sorted(categories.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "def get_directions(centroids, L=1):\n",
    "    directions = [None] * (len(centroids) - 1)\n",
    "    for i in range(0, len(centroids) - L, L):\n",
    "        direction = centroids[i + L] - centroids[i]\n",
    "        for j in range(i, i + L):\n",
    "            directions[j] = direction\n",
    "    # Fill the remaining slots with the last calculated direction\n",
    "    for i in range(len(centroids) - L, len(centroids) - 1):\n",
    "        directions[i] = direction\n",
    "    return np.array(directions)\n",
    "\n",
    "def get_most_common(top_n_frames):\n",
    "    return Counter(top_n_frames).most_common(1)[0][0]\n",
    "\n",
    "def get_sorted_direction(sorted_moving_directions, n=2):\n",
    "\n",
    "    first_direction = sorted_moving_directions[0][0]\n",
    "    moving_direction_name = first_direction[0]\n",
    "    \n",
    "    directions = [moving_direction_name]\n",
    "    top_n_directions = []\n",
    "    for i in range(len(sorted_moving_directions)):\n",
    "        top_n_direction = sorted_moving_directions[i][:n]\n",
    "        top_n_directions.append(top_n_direction)\n",
    "        top_n_direction_names = [direction[0] for direction in top_n_direction]\n",
    "        if moving_direction_name not in top_n_direction_names:\n",
    "            moving_direction_name = top_n_direction[0][0]\n",
    "        directions.append(moving_direction_name)\n",
    "    return np.array(directions), top_n_directions\n",
    "\n",
    "def get_nonzero_top_n_directions(frames, idxs=None, L = 1, n=5, stationary_threshold_ratio=1.6, dimensions=2, reverse=False, avg_distance=None):\n",
    "    if idxs is None:\n",
    "        idxs = [i for i in range(1, 22)]\n",
    "    frames = frames[::-1] if reverse else frames\n",
    "    frames = [frame[idxs] for frame in frames]\n",
    "    centroids = np.array([np.mean(frame, axis=0) for frame in frames])\n",
    "    direction_vecs = get_directions(centroids)\n",
    "    distances = np.array([np.linalg.norm(direction) for direction in direction_vecs])\n",
    "    avg_distance = np.mean(distances) if avg_distance is None else avg_distance\n",
    "    sorted_moving_directions = [moving_directions_sorted(direction_vec, avg_distance, stationary_threshold_ratio=stationary_threshold_ratio, dimensions=dimensions) for direction_vec in direction_vecs]\n",
    "    directions, top_n_directions = get_sorted_direction(sorted_moving_directions, n)\n",
    "    nonzero_top_n_directions = [[d for d in direction if abs(d[1]) > 0] for direction in top_n_directions]\n",
    "    # add stationary direction to the second element of the list\n",
    "    return [[('stationary', 0)]] + nonzero_top_n_directions\n",
    "\n",
    "def exponential_decay(d):\n",
    "    return 2 ** (-d)\n",
    "\n",
    "def calculate_weights(distances, formula=lambda d: 1 / (d + 1)):\n",
    "    return [formula(d) for d in distances]\n",
    "\n",
    "def sort_key(item):\n",
    "    direction, slope = item\n",
    "    if direction in ['left', 'down', 'away_from_camera']:\n",
    "        return -slope  # For 'left' and 'down', a negative slope is considered larger\n",
    "    else:\n",
    "        return slope  # For other directions, a positive slope is considered larger\n",
    "    \n",
    "def get_motion_directions(frame_data, top_dirs=1, window_size=3, weight_formula=exponential_decay):\n",
    "\n",
    "    output = []\n",
    "    distances = [0 for _ in frame_data]\n",
    "    distances_history = [dict(left=0, right=0, up=0, down=0, stationary=0) for _ in frame_data]\n",
    "    weighted_distances_history = [dict(left=0, right=0, up=0, down=0, stationary=0) for _ in frame_data]\n",
    "    \n",
    "    for i in range(len(frame_data)):\n",
    "        frame = frame_data[i]\n",
    "\n",
    "        if not frame:\n",
    "            if i == 0:\n",
    "                output.append('stationary')\n",
    "            else:\n",
    "                if len(output) < (window_size//2 + 1):\n",
    "                    prev_frames = frame_data[max(0, i-window_size//2):i if i > 0 else 1]\n",
    "                else:\n",
    "                    # use the distances_history to calculate the previous frames\n",
    "                    prev_distances = distances_history[max(0, i-window_size//2):i].copy()\n",
    "                    prev_distances = [{k: v for k, v in sorted(prev_distance.items(), key=sort_key, reverse=True)} for prev_distance in prev_distances]\n",
    "                    prev_frames = [list(prev_distance.items()) for prev_distance in prev_distances]\n",
    "                \n",
    "                prev_frames = [f for f in prev_frames if f]\n",
    "                        \n",
    "                next_frames = frame_data[i+1:min(len(frame_data), i+window_size//2+1)]\n",
    "                next_frames = [f for f in next_frames if f]\n",
    "                \n",
    "                prev_distances = range(len(prev_frames), 0, -1)\n",
    "                next_distances = range(1, len(next_frames) + 1)\n",
    "                \n",
    "                prev_weights = calculate_weights(prev_distances, weight_formula)\n",
    "                next_weights = calculate_weights(next_distances, weight_formula)\n",
    "\n",
    "                dir_weights = defaultdict(int)\n",
    "                for prev_frame, weight in zip(prev_frames, prev_weights):\n",
    "                    for dir, dist in prev_frame[:top_dirs]:\n",
    "                        dir_weights[dir] += weight * dist\n",
    "                        distances_history[i][dir] += dist\n",
    "                        weighted_distances_history[i][dir] += weight * dist\n",
    "                    \n",
    "                for next_frame, weight in zip(next_frames, next_weights):\n",
    "                    for dir, dist in next_frame[:top_dirs]:\n",
    "                        dir_weights[dir] += weight * dist\n",
    "                        distances_history[i][dir] += dist\n",
    "                        weighted_distances_history[i][dir] += weight * dist\n",
    "\n",
    "                common_dir = max(dir_weights, key=lambda x: abs(dir_weights[x]))\n",
    "                output.append(common_dir)\n",
    "                distances.append(dir_weights[common_dir])\n",
    "                for prev_frame in prev_frames:\n",
    "                    for dir, dist in prev_frame:\n",
    "                        if dir == common_dir:\n",
    "                            distances[i] += dist\n",
    "                for next_frame in next_frames:\n",
    "                    for dir, dist in next_frame:\n",
    "                        if dir == common_dir:\n",
    "                            distances[i] += dist\n",
    "\n",
    "        else:\n",
    "            if frame[0][0] == 'stationary' and len(frame) == 1:\n",
    "                if len(output) < (window_size//2 + 1):\n",
    "                    prev_frames = frame_data[max(0, i-window_size//2):i if i > 0 else 1]\n",
    "                else:\n",
    "                    # use the distances_history to calculate the previous frames\n",
    "                    prev_distances = distances_history[max(0, i-window_size//2):i].copy()\n",
    "                    prev_distances = [{k: v for k, v in sorted(prev_distance.items(), key=sort_key, reverse=True)} for prev_distance in prev_distances]\n",
    "                    prev_frames = [list(prev_distance.items()) for prev_distance in prev_distances]\n",
    "\n",
    "                prev_frames = [f for f in prev_frames if f]\n",
    "                next_frames = frame_data[i+1:min(len(frame_data), i+window_size//2+1)]\n",
    "                next_frames = [f for f in next_frames if f]\n",
    "                prev_distances = range(len(prev_frames), 0, -1)\n",
    "                next_distances = range(1, len(next_frames) + 1)\n",
    "                \n",
    "                prev_weights = calculate_weights(prev_distances, weight_formula)\n",
    "                next_weights = calculate_weights(next_distances, weight_formula)\n",
    "\n",
    "                dir_weights = defaultdict(int)\n",
    "                for prev_frame, weight in zip(prev_frames, prev_weights):\n",
    "                    for dir, dist in prev_frame[:top_dirs]:\n",
    "                        dir_weights[dir] += weight\n",
    "                        distances_history[i][dir] += dist\n",
    "                        weighted_distances_history[i][dir] += weight * dist\n",
    "\n",
    "                for next_frame, weight in zip(next_frames, next_weights):\n",
    "                    for dir, dist in next_frame[:top_dirs]:\n",
    "                        dir_weights[dir] += weight\n",
    "                        distances_history[i][dir] += dist\n",
    "                        weighted_distances_history[i][dir] += weight * dist\n",
    "\n",
    "                common_dir = max(dir_weights, key=lambda x: abs(dir_weights[x]))\n",
    "                output.append(common_dir)\n",
    "                for prev_frame in prev_frames:\n",
    "                    for dir, dist in prev_frame:\n",
    "                        if dir == common_dir:\n",
    "                            distances[i] += dist\n",
    "                for next_frame in next_frames:\n",
    "                    for dir, dist in next_frame:\n",
    "                        if dir == common_dir:\n",
    "                            distances[i] += dist\n",
    "            else:\n",
    "                if i == 0 or not output[-1]:\n",
    "                    output.append(frame[0][0])\n",
    "                else:\n",
    "                    if len(output) < (window_size//2 + 1):\n",
    "                        prev_frames = frame_data[max(0, i-window_size//2):i if i > 0 else 1]\n",
    "                    else:\n",
    "                        # use the distances_history to calculate the previous frames\n",
    "                        prev_distances = distances_history[max(0, i-window_size//2):i].copy()\n",
    "                        # sort the distances_history by the absolute value of the distance\n",
    "                        prev_distances = [{k: v for k, v in sorted(prev_distance.items(), key=sort_key, reverse=True)} for prev_distance in prev_distances]\n",
    "                        prev_frames = [list(prev_distance.items()) for prev_distance in prev_distances]\n",
    "                    prev_top_dir = output[-1]\n",
    "                    next_frames = frame_data[i+1:min(len(frame_data), i+window_size//2+1)]\n",
    "                    if prev_top_dir in [dir for dir, _ in frame[:top_dirs]]:\n",
    "                        output.append(prev_top_dir)\n",
    "                    elif len(next_frames) > 0 and prev_top_dir in [dir for dir, _ in next_frames[0][:top_dirs]]:\n",
    "                        output.append(prev_top_dir)\n",
    "                    else:\n",
    "                        output.append(frame[0][0])\n",
    "\n",
    "                    for prev_frame in prev_frames:\n",
    "                        for dir, dist in prev_frame:\n",
    "                            distances_history[i][dir] += dist\n",
    "                            weighted_distances_history[i][dir] += dist\n",
    "                            if dir == output[-1]:\n",
    "                                distances[i] += dist\n",
    "                            \n",
    "                    for next_frame in next_frames:\n",
    "                        for dir, dist in next_frame:\n",
    "                            distances_history[i][dir] += dist\n",
    "                            weighted_distances_history[i][dir] += dist\n",
    "                            if dir == output[-1]:\n",
    "                                distances[i] += dist\n",
    "\n",
    "    # Sort each dictionary in each of the history lists by absolute value (descending)\n",
    "    for i in range(len(distances_history)):\n",
    "        distances_history[i] = {k: v for k, v in sorted(distances_history[i].items(), key=sort_key, reverse=True)}\n",
    "        weighted_distances_history[i] = {k: v for k, v in sorted(weighted_distances_history[i].items(), key=sort_key, reverse=True)}\n",
    "    return output, distances, distances_history, weighted_distances_history\n",
    "\n",
    "def get_frames_data(frames):\n",
    "    return np.array([frames[i] for i in range(len(frames))])\n",
    "\n",
    "def get_previous_frames_data(n, frames):\n",
    "    return get_frames_data(frames[-n:])\n",
    "\n",
    "def get_frames_directions(frames, idxs = None, L = 1, n=5, stationary_threshold_ratio=1.6, avg_distance = None):\n",
    "    return get_nonzero_top_n_directions(get_frames_data(frames), idxs = idxs, L = L, n = n, stationary_threshold_ratio = stationary_threshold_ratio, avg_distance = avg_distance)\n",
    "\n",
    "def get_frames_motion(frames_directions = None, top_dirs = 2, window_size = WINDOW_SIZE, weight_formula = exponential_decay, idxs = None, L = 1, n=5, stationary_threshold_ratio=1.6, avg_distance = None):\n",
    "    frames_directions = get_frames_directions(idxs = idxs, L = L, n = n, stationary_threshold_ratio = stationary_threshold_ratio, avg_distance = avg_distance) if frames_directions is None else frames_directions\n",
    "    return get_motion_directions(frames_directions, top_dirs = top_dirs, window_size = window_size, weight_formula = weight_formula)\n",
    "\n",
    "def get_previous_frames_motion(frames, n, top_dirs = 2, window_size = WINDOW_SIZE, weight_formula = exponential_decay, idxs = None, L = 1, stationary_threshold_ratio=1.6, avg_distance = None):\n",
    "    return get_frames_motion(get_frames_directions(frames[-n:], idxs = idxs, L = L, stationary_threshold_ratio = stationary_threshold_ratio), top_dirs = top_dirs, window_size = window_size, weight_formula = weight_formula, avg_distance=avg_distance)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 **Calculating** the moving direction for each frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames 0:4 -> stationary\n",
      "\t Hand motion transition from: stationary -> left at frame 4\n",
      "\n",
      "Frames 4:26 -> left\n",
      "\t Hand motion transition from: left -> stationary at frame 26\n",
      "\n",
      "Frames 26:41 -> stationary\n",
      "\t Hand motion transition from: stationary -> right at frame 41\n",
      "\n",
      "Frames 41:85 -> right\n",
      "\t Hand motion transition from: right -> stationary at frame 85\n",
      "\n",
      "Frames 85:105 -> stationary\n",
      "\t Hand motion transition from: stationary -> left at frame 105\n",
      "\n",
      "Frames 105:158 -> left\n",
      "\t Hand motion transition from: left -> stationary at frame 158\n",
      "\n",
      "Frames 158:181 -> stationary\n",
      "\t Hand motion transition from: stationary -> right at frame 181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_dirs = 3\n",
    "window_size = 3\n",
    "stationary_threshold = 1.3 # 1.418\n",
    "moving_direction_indexes = [6, 18, 10, 14, 0, 1, 3, 7, 19] # Indexes of the landmarks to use for calculating the moving directions\n",
    "L = 1\n",
    "direction_mapping_2d = {'up': 0, 'down': 1, 'left': 2, 'right': 3, 'stationary': 4}\n",
    "frames = swipe_right_trial[\"raw\"]\n",
    "window_size_moving_directions = get_previous_frames_motion(frames=frames, n = len(frames), top_dirs = top_dirs, window_size = window_size, weight_formula = exponential_decay, idxs = moving_direction_indexes, L = L, stationary_threshold_ratio = stationary_threshold)\n",
    "prev_state = \"stationary\"\n",
    "prev_frame_change = 0\n",
    "\n",
    "interval_dict = {}\n",
    "for i, state in enumerate(window_size_moving_directions):\n",
    "    if prev_state is not None:\n",
    "        if state != prev_state:\n",
    "            interval_dict[(prev_frame_change, i)] = [prev_state, state]\n",
    "            print(f\"Frames {prev_frame_change}:{i} -> {prev_state}\")\n",
    "\n",
    "            print(f\"\\t Hand motion transition from: {prev_state} -> {state} at frame {i}\\n\")\n",
    "\n",
    "            prev_frame_change = i\n",
    "\n",
    "\n",
    "    prev_state = state\n",
    "\n",
    "last_interval = list(interval_dict.keys())[-1]\n",
    "last_state = interval_dict[last_interval][1]\n",
    "interval_dict[(last_interval[1], len(frames)-1)] = [last_state, last_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 **Visualising** the moving directions over frames:\n",
    "- As a sanity check we can redraw the hand landmarks over the frames to see if the discretised moving directions are accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 1: 0:4 -> stationary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb8214ca41745acb620198285f8793d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', layout=Layout(width='50%'), max=3), Output()), _…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 2: 4:26 -> left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d93cac8509f41c79ceacfc0b869ffec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='Frame', layout=Layout(width='50%'), max=25, min=4), Outp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 3: 26:41 -> stationary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8783b5120019466db49f047eb990512c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=26, description='Frame', layout=Layout(width='50%'), max=40, min=26), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 4: 41:85 -> right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdcca9f6839434eb13c4d0ecb42792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=41, description='Frame', layout=Layout(width='50%'), max=84, min=41), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 5: 85:105 -> stationary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd239dd87b4846468fa85576317552ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=85, description='Frame', layout=Layout(width='50%'), max=104, min=85), O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 6: 105:158 -> left\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190bfd13d95541a0b04e4b47225264b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=105, description='Frame', layout=Layout(width='50%'), max=157, min=105),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 7: 158:181 -> stationary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430ff6771f4142a89cf9fa12a497138c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=158, description='Frame', layout=Layout(width='50%'), max=180, min=158),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval 8: 181:225 -> right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c6422c107e49c897a59c51ea9c608b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=181, description='Frame', layout=Layout(width='50%'), max=224, min=181),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import cv2\n",
    "\n",
    "from ultraleap_demo.hand_renderer import HandRenderer # This is a simple class to render the recorded hand gesture data frame by frame\n",
    "\n",
    "# Load the hand data\n",
    "hand_data = swipe_right_trial[\"hands\"]\n",
    "hands_colour = (255, 255, 255)\n",
    "\n",
    "# Create the instances of the HandRenderer class for dots and skeleton formats\n",
    "dots_renderer = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Dots\", hands_colour=hands_colour, circle_radius=3)\n",
    "skeleton_renderer = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Skeleton\", hands_colour=hands_colour, circle_radius=3)\n",
    "\n",
    "for i in range(len(list(interval_dict.keys()))):\n",
    "    start, end = list(interval_dict.keys())[i]\n",
    "    print(f\"Interval {i+1}: {start}:{end} -> {interval_dict[(start, end)][0]}\")\n",
    "    frame_slider = widgets.IntSlider(min=start, max=end-1, step=1, value=start, layout=widgets.Layout(width='50%'), description='Frame')\n",
    "    def show_frames(frame_number):\n",
    "        # Render the hand data for this frame\n",
    "        dots_renderer.render_hand_data(hand_data[frame_number])\n",
    "        skeleton_renderer.render_hand_data(hand_data[frame_number])\n",
    "\n",
    "        # Convert the images from BGR to RGB\n",
    "        dots_image_rgb = cv2.cvtColor(dots_renderer.output_image, cv2.COLOR_BGR2RGB)\n",
    "        skeleton_image_rgb = cv2.cvtColor(skeleton_renderer.output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "\n",
    "        # Display the dots image\n",
    "        plt.imshow(skeleton_image_rgb)\n",
    "        plt.axis('off')  # Hide the axes\n",
    "        plt.title(\"Skeleton\")  # Set the title\n",
    "\n",
    "        # Display the skeleton image\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(dots_image_rgb)\n",
    "        plt.axis('off')  # Hide the axes\n",
    "        plt.title(\"Dots\")  # Set the title\n",
    "        plt.suptitle(f\"Frame {frame_number} - Motion: {window_size_moving_directions[frame_number]}\", fontsize=16)\n",
    "        # decrease vertical space between subplots and suptitle\n",
    "        plt.subplots_adjust(top=1.1)\n",
    "\n",
    "        plt.show()\n",
    "    # Display the slider and the frame\n",
    "    interact(show_frames, frame_number=frame_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 **Hand Pose**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 **Functions** to calculate the hand pose similarity using hand joint bending angles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ultraleap_demo.classes import HandPose\n",
    "\n",
    "\n",
    "def get_bending_joint_idxs():\n",
    "    return [\n",
    "        (0,2,4),(2,4,5), # thumb\n",
    "        (0,6,8),(6,8,9), # index\n",
    "        (0,10,12),(10,12,13), # middle\n",
    "        (0,14,16),(14,16,17), # ring\n",
    "        (0,18,20),(18,20,21) # little\n",
    "    ]\n",
    "\n",
    "def get_bending_angles(jointA, jointB, jointC):\n",
    "    # Calculate vectors\n",
    "    AB = np.subtract(jointA, jointB)\n",
    "    BC = np.subtract(jointC, jointB)\n",
    "\n",
    "    # Calculate dot product and norms\n",
    "    dot_product = np.dot(AB, BC)\n",
    "    norm_ab = np.linalg.norm(AB)\n",
    "    norm_bc = np.linalg.norm(BC)\n",
    "\n",
    "    # Calculate angle in radians and make sure no division by zero\n",
    "    angle_rad = np.arccos(dot_product / (norm_ab * norm_bc + 1e-6))\n",
    "\n",
    "    # Convert to degrees\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    return angle_deg\n",
    "\n",
    "def joint_bending_angles(coordinates):\n",
    "    bending_joint_idxs = get_bending_joint_idxs()\n",
    "    bending_angles = []\n",
    "    for idxs in bending_joint_idxs:\n",
    "        jointA = np.array(coordinates[idxs[0]])\n",
    "        jointB = np.array(coordinates[idxs[1]])\n",
    "        jointC = np.array(coordinates[idxs[2]])\n",
    "\n",
    "        # Calculate the bending angle\n",
    "        angle = get_bending_angles(jointA, jointB, jointC)\n",
    "        bending_angles.append(angle)\n",
    "    return np.array(bending_angles)\n",
    "\n",
    "def evaluate_similarity_score(score, threshold = 0.5):\n",
    "    if score < threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 **Calculating** the hand pose similarity for each frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_lookback = 5\n",
    "similarity_threshold = 0.9\n",
    "\n",
    "frames = swipe_right_trial[\"raw\"]\n",
    "\n",
    "handpose_model_dir = os.path.join(os.path.join(os.getcwd(), 'ultraleap_demo'), 'handpose_models')\n",
    "handpose_model_paths = {os.path.join(handpose_model_dir, p).split(\"\\\\\")[-1].split(\".\")[0].split(\"_\")[0]:os.path.join(handpose_model_dir, p) for p in os.listdir(handpose_model_dir)}\n",
    "\n",
    "handpose = HandPose(handpose_model_paths[\"filtereddhg\"])\n",
    "similarities = []\n",
    "evaluated_scores = []\n",
    "for i in range(len(frames) - similarity_lookback):\n",
    "    lookback_frame = frames[i]\n",
    "    current_frame = frames[i + similarity_lookback]\n",
    "\n",
    "    lookback_frame_bending_angles = joint_bending_angles(lookback_frame)\n",
    "    current_frame_bending_angles = joint_bending_angles(current_frame)\n",
    "\n",
    "    similarity_score = handpose.get_similarity(current_frame_bending_angles, lookback_frame_bending_angles)\n",
    "    evaluated_score = evaluate_similarity_score(similarity_score, threshold = similarity_threshold)\n",
    "\n",
    "    similarities.append(similarity_score)\n",
    "    evaluated_scores.append(evaluated_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 **Visualising** the hand pose similarity over frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da15609afd54df0889ae919f74d839f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', layout=Layout(width='90%'), max=195), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_frames(frame_number)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import cv2\n",
    "\n",
    "from ultraleap_demo.hand_renderer import HandRenderer # This is a simple class to render the recorded hand gesture data frame by frame\n",
    "\n",
    "# Load the hand data\n",
    "hand_data = swipe_right_trial[\"hands\"]\n",
    "hands_colour = (255, 255, 255)\n",
    "\n",
    "# Create the instances of the HandRenderer class for dots and skeleton formats\n",
    "skeleton_renderer_lookback = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Skeleton\", hands_colour=hands_colour, circle_radius=3)\n",
    "skeleton_renderer_current = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Skeleton\", hands_colour=hands_colour, circle_radius=3)\n",
    "\n",
    "# Create a slider for frame navigation\n",
    "frame_slider = widgets.IntSlider(min=0, max=len(hand_data)-1-similarity_lookback, step=1, value=0, layout=widgets.Layout(width='90%'), description='Frame')\n",
    "\n",
    "def show_frames(frame_number):\n",
    "    # Render the hand data for this frame\n",
    "    skeleton_renderer_lookback.render_hand_data(hand_data[frame_number])\n",
    "    skeleton_renderer_current.render_hand_data(hand_data[frame_number + similarity_lookback])\n",
    "\n",
    "    # Convert the images from BGR to RGB\n",
    "    current_image_rgb = cv2.cvtColor(skeleton_renderer_lookback.output_image, cv2.COLOR_BGR2RGB)\n",
    "    lookback_image_rgb = cv2.cvtColor(skeleton_renderer_current.output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Display the dots image\n",
    "    plt.imshow(current_image_rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(f\"Current - Frame {frame_number}\")  # Set the title\n",
    "\n",
    "    # Display the skeleton image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(lookback_image_rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(f\"Lookback - Frame {frame_number + similarity_lookback}\")  # Set the title\n",
    "\n",
    "    plt.suptitle(f\"Hand Pose Similarity: {similarities[frame_number]:.4f} -> {'Similar' if evaluated_scores[frame_number] == 0 else 'Dissimilar'}\", fontsize=16)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display the slider and the frame\n",
    "interact(show_frames, frame_number=frame_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Classifying dynamic hand gestures using the sequences of discrete features (optional)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure all features are same length as similarity scores (due to lookback window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of moving directions: 226\n",
      "Length of palm orientations: 226\n",
      "Length of similarity scores: 221\n",
      "\n",
      "Length of moving directions: 221\n",
      "Length of palm orientations: 221\n",
      "Length of similarity scores: 221\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of moving directions: {len(window_size_moving_directions)}\")\n",
    "print(f\"Length of palm orientations: {len(palm_orientation_features)}\")\n",
    "print(f\"Length of similarity scores: {len(evaluated_scores)}\\n\")\n",
    "\n",
    "moving_directions = window_size_moving_directions[similarity_lookback:]\n",
    "palm_orientations = palm_orientation_features[similarity_lookback:]\n",
    "evaluated_scores = evaluated_scores\n",
    "\n",
    "print(f\"Length of moving directions: {len(moving_directions)}\")\n",
    "print(f\"Length of palm orientations: {len(palm_orientations)}\")\n",
    "print(f\"Length of similarity scores: {len(evaluated_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the feature labels to integer labels for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 moving directions ['left', 'left', 'left', 'left', 'left']\n",
      "First 5 palm orientations ['down', 'down', 'down', 'down', 'down']\n",
      "First 5 hand pose scores [0, 0, 0, 0, 0]\n",
      "\n",
      "First 5 moving directions [2, 2, 2, 2, 2]\n",
      "First 5 palm orientations [1, 1, 1, 1, 1]\n",
      "First 5 hand pose scores [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "orientation_mapping = {'up': 0, 'down': 1, 'opposite': 2}\n",
    "inverted_orientation_mapping = {v: k for k, v in orientation_mapping.items()}\n",
    "direction_mapping_2d = {'up': 0, 'down': 1, 'left': 2, 'right': 3, 'stationary': 4}\n",
    "inverted_direction_mapping_2d = {v: k for k, v in direction_mapping_2d.items()}\n",
    "\n",
    "print(f\"First 5 moving directions {moving_directions[:5]}\")\n",
    "print(f\"First 5 palm orientations {palm_orientations[:5]}\")\n",
    "print(f\"First 5 hand pose scores {evaluated_scores[:5]}\\n\")\n",
    "\n",
    "int_moving_directions = [direction_mapping_2d[direction] for direction in moving_directions]\n",
    "int_palm_orientations = [orientation_mapping[orientation] for orientation in palm_orientations]\n",
    "int_hand_pose = evaluated_scores\n",
    "\n",
    "print(f\"First 5 moving directions {int_moving_directions[:5]}\")\n",
    "print(f\"First 5 palm orientations {int_palm_orientations[:5]}\")\n",
    "print(f\"First 5 hand pose scores {int_hand_pose[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remapping the feature labels to same labels that were used for training the Transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mapping_dir = os.path.join('ultraleap_demo','state_mapping')\n",
    "\n",
    "# load the state mapping for each of the features\n",
    "moving_direction_state_mapping = pickle.load(open(os.path.join(state_mapping_dir, 'moving_direction_mapping.pkl'), 'rb'))\n",
    "inverted_moving_direction_state_mapping = {v: k for k, v in moving_direction_state_mapping.items()}\n",
    "palm_orientation_state_mapping = pickle.load(open(os.path.join(state_mapping_dir, 'palm_orientation_mapping.pkl'), 'rb'))\n",
    "inverted_palm_orientation_state_mapping = {v: k for k, v in palm_orientation_state_mapping.items()}\n",
    "\n",
    "mapped_moving_directions = np.array([moving_direction_state_mapping[str(direction)] for direction in int_moving_directions])\n",
    "mapped_palm_orientations = np.array([palm_orientation_state_mapping[str(orientation)] for orientation in int_palm_orientations])\n",
    "mapped_hand_pose = np.array(int_hand_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note:** Follow this process (creating feature vector for each frame) for many different hand gesture examples in order to create the lookup table for classification.\n",
    "- Below is an example of how to load the lookup table and prepare it to be used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle files...\n",
      "Done loading pickle files\n"
     ]
    }
   ],
   "source": [
    "from ultraleap_demo.load_demo import *\n",
    "from data.dhg import DHG\n",
    "\n",
    "test_size = 0.1 # The size of the test set (Smaller pct means slower classification but more accuracy. Larger pct means faster classification but less accuracy)\n",
    "target_length = 64 # The target length to normalize the sequences to\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "dhg_data = DHG(data_dir)\n",
    "\n",
    "performer_states_dict = get_performer_states_dict(dhg_data, smoother=True)\n",
    "normalized_performer_states_dict = normalize_performer_state(performer_states_dict, by_subject=True)\n",
    "gestures = [f\"gesture_{i}\" for i in range(7, 11)]\n",
    "train_lookup_table, test_lookup_table = make_train_test_lookup_table(normalized_performer_states_dict, test_size=test_size, gestures=gestures)\n",
    "\n",
    "normalized_train_lookup_table = normalize_lookup_table(train_lookup_table, target_length)\n",
    "prepared_train_lookup_table = lookup_table_tensor(normalized_train_lookup_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to make Transformer time series prediction, combine the prediction to the original sequence and then classify this new sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultraleap_demo.load_demo import model_dict\n",
    "\n",
    "def classify_gesture_sequence(mapped_moving_directions, mapped_palm_orientations, mapped_hand_poses, sequence_length, output_window, lookup_table, target_length, threshold=0.05, device='cpu'):\n",
    "    gesture_mapped_names = {\n",
    "        \"gesture_7\":\"Swipe Right\",\n",
    "        \"gesture_8\":\"Swipe Left\",\n",
    "        \"gesture_9\":\"Swipe Up\",\n",
    "        \"gesture_10\":\"Swipe Down\",\n",
    "    }\n",
    "\n",
    "    combined = np.squeeze(combine_mapped_separated_sequences([mapped_moving_directions], [mapped_palm_orientations], [mapped_hand_poses]), axis=1)[0].T\n",
    "    to_predict_sequence = make_predict_frame_sequence(list(combined), sequence_length, output_window)\n",
    "    model_dict[sequence_length][output_window] = model_dict[sequence_length][output_window].to(device)\n",
    "    model = model_dict[sequence_length][output_window]\n",
    "    model.eval()\n",
    "    to_predict_sequence = to_predict_sequence.to(device)\n",
    "    \n",
    "    predicted_states = combine_predicted_features(make_prediction(model, to_predict_sequence, output_window))\n",
    "    performed_states = to_predict_sequence.tolist()[0][:-output_window]\n",
    "    classify_sequence = performed_states + predicted_states\n",
    "\n",
    "    gesture, _, scores = classify_gesture(classify_sequence=classify_sequence, lookup_table=lookup_table, target_length=target_length)\n",
    "\n",
    "    gesture_scores = [l[1] for l in list(scores.values())]\n",
    "    # Calculate the sum of all scores\n",
    "    total_score = sum(gesture_scores)\n",
    "\n",
    "    # Normalize the scores\n",
    "    normalized_scores = [score / total_score for score in gesture_scores]\n",
    "\n",
    "    # Sort the normalized scores in descending order\n",
    "    sorted_scores = sorted(normalized_scores, reverse=True)\n",
    "\n",
    "    # Calculate the difference between the best score and the second best score\n",
    "    score_difference = sorted_scores[0] - sorted_scores[1]\n",
    "\n",
    "    # Check if the score difference is below the threshold\n",
    "    if score_difference < threshold:\n",
    "        classified_gesture = \"Unknown\"\n",
    "    else:\n",
    "        classified_gesture = gesture_mapped_names[gesture]\n",
    "\n",
    "    return classified_gesture, score_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying the hand gesture sequences combined with the Transformer time series predictions using the lookup table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import cv2\n",
    "\n",
    "from ultraleap_demo.hand_renderer import HandRenderer # This is a simple class to render the recorded hand gesture data frame by frame\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = len(mapped_moving_directions)\n",
    "\n",
    "confidence_threshold = 0.05\n",
    "sequence_length = 32\n",
    "target_length = 64\n",
    "output_window = 1\n",
    "device = 'cpu'\n",
    "normalize = True\n",
    "\n",
    "intervals = []\n",
    "classified_gestures = []\n",
    "confidences = []\n",
    "for i in range(1, end_idx):\n",
    "    if i - start_idx >= target_length:\n",
    "        start_idx += 1 \n",
    "    mapped_moving_direction_sequence = mapped_moving_directions[start_idx:i]\n",
    "    mapped_palm_orientation_sequence = mapped_palm_orientations[start_idx:i]\n",
    "    mapped_hand_pose_sequence = mapped_hand_pose[start_idx:i]\n",
    "\n",
    "    if normalize:\n",
    "        mapped_moving_direction_sequence = normalize_sequence(mapped_moving_direction_sequence, target_length)\n",
    "        mapped_palm_orientation_sequence = normalize_sequence(mapped_palm_orientation_sequence, target_length)\n",
    "        mapped_hand_pose_sequence = normalize_sequence(mapped_hand_pose_sequence, target_length)\n",
    "\n",
    "    classified_gesture,confidence = classify_gesture_sequence(mapped_moving_direction_sequence, mapped_palm_orientation_sequence, mapped_hand_pose_sequence, sequence_length, output_window, prepared_train_lookup_table, target_length, threshold=confidence_threshold, device=device)\n",
    "\n",
    "    classified_gestures.append(classified_gesture)\n",
    "    confidences.append(confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualising** the classification results:\n",
    "- Confidence increases as the model sees more frames of the hand gesture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd2c48296df4558b0454e55fe48334e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', layout=Layout(width='90%'), max=219), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_frames(frame_number)>"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import cv2\n",
    "\n",
    "from ultraleap_demo.hand_renderer import HandRenderer # This is a simple class to render the recorded hand gesture data frame by frame\n",
    "\n",
    "# Load the hand data\n",
    "hand_data = swipe_right_trial[\"hands\"]\n",
    "hands_colour = (255, 255, 255)\n",
    "\n",
    "# Create the instances of the HandRenderer class for dots and skeleton formats\n",
    "dots_renderer = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Dots\", hands_colour=hands_colour, circle_radius=3)\n",
    "skeleton_renderer = HandRenderer(np.zeros((480, 640, 3), dtype=np.uint8), hands_format=\"Skeleton\", hands_colour=hands_colour, circle_radius=3)\n",
    "\n",
    "frame_slider = widgets.IntSlider(min=0, max=end_idx-1-output_window, step=1, value=0, layout=widgets.Layout(width='90%'), description='Frame')\n",
    "def show_frames(frame_number):\n",
    "    # Render the hand data for this frame\n",
    "    dots_renderer.render_hand_data(hand_data[frame_number])\n",
    "    skeleton_renderer.render_hand_data(hand_data[frame_number])\n",
    "\n",
    "    # Convert the images from BGR to RGB\n",
    "    dots_image_rgb = cv2.cvtColor(dots_renderer.output_image, cv2.COLOR_BGR2RGB)\n",
    "    skeleton_image_rgb = cv2.cvtColor(skeleton_renderer.output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Display the dots image\n",
    "    plt.imshow(skeleton_image_rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(\"Skeleton\")  # Set the title\n",
    "\n",
    "    # Display the skeleton image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(dots_image_rgb)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(\"Dots\")  # Set the title\n",
    "    plt.suptitle(f\"Frame {frame_number} - Classified Gesture: {classified_gestures[frame_number]} with a confidence of {confidences[frame_number]:.4f}\", fontsize=16)\n",
    "    # decrease vertical space between subplots and suptitle\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display the slider and the frame\n",
    "interact(show_frames, frame_number=frame_slider)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
